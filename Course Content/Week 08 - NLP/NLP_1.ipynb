{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"Tokenization is the process of breaking down a text into individual words or tokens.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzAhD4Py6rqf",
        "outputId": "98718da0-6ccf-4f60-dc54-900760b9e12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming using NLTK:**"
      ],
      "metadata": {
        "id": "x4jk41KR8B4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Sample words to stem\n",
        "words = [\"running\", \"flies\", \"jumping\", \"friendly\"]\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem the words\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Print the stemmed words\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0TRaZjs6roD",
        "outputId": "1033273d-7635-42c9-9183-06c0ad430548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'jump', 'friendli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization using spaCy:**"
      ],
      "metadata": {
        "id": "Jco_DH8-8PKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample words to lemmatize\n",
        "words = [\"running\", \"flies\", \"jumping\", \"friendly\"]\n",
        "\n",
        "# Lemmatize the words\n",
        "lemmatized_words = [token.lemma_ for token in nlp(\" \".join(words))]\n",
        "\n",
        "# Print the lemmatized words\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CxQ-IxB6rlh",
        "outputId": "e0f20c9e-03a7-47fc-bf0b-64bd74add5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fly', 'jump', 'friendly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Normalization:**"
      ],
      "metadata": {
        "id": "fZJNKklHEzaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text with variations\n",
        "text = \"I luv metapi. Plz gr8 job! AI is amazin'.\"\n",
        "\n",
        "# Define a simple text normalization function\n",
        "def text_normalization(text):\n",
        "    # Replace abbreviations and acronyms\n",
        "    text = text.replace(\"luv\", \"love\")\n",
        "    text = text.replace(\"Plz\", \"Please\")\n",
        "    text = text.replace(\"gr8\", \"great\")\n",
        "    text = text.replace(\"amazin'\", \"amazing\")\n",
        "\n",
        "    # Convert to lowercase (optional)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply text normalization\n",
        "normalized_text = text_normalization(text)\n",
        "\n",
        "# Print the normalized text\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbEQWS1XD4N2",
        "outputId": "cc1d0b99-b7b4-44dc-97ef-c263653a7cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love metapi. please great job! ai is amazing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Redundancy**\n"
      ],
      "metadata": {
        "id": "VnjkqUXXH0b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "# Sample list of text entries with potential duplicates\n",
        "text_entries = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"Another example sentence.\",\n",
        "    \"This is an example sentence.\",  # Duplicate\n",
        "    \"Yet another example sentence.\",\n",
        "    \"Some unique text here.\",\n",
        "    \"This is an example sentence.\"   # Duplicate\n",
        "]\n",
        "\n",
        "# Function to remove duplicates based on similarity threshold\n",
        "def remove_redundancy(text_entries, similarity_threshold=0.8):\n",
        "    # Initialize a list to store the deduplicated entries\n",
        "    deduplicated_entries = []\n",
        "\n",
        "    # Iterate through the text entries\n",
        "    for entry in text_entries:\n",
        "        # Flag to check if the entry is a duplicate\n",
        "        is_duplicate = False\n",
        "\n",
        "        # Compare the entry with each deduplicated entry\n",
        "        for dedup_entry in deduplicated_entries:\n",
        "            similarity = difflib.SequenceMatcher(None, entry, dedup_entry).ratio()\n",
        "            if similarity >= similarity_threshold:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        # If the entry is not a duplicate, add it to the deduplicated list\n",
        "        if not is_duplicate:\n",
        "            deduplicated_entries.append(entry)\n",
        "\n",
        "    return deduplicated_entries\n",
        "\n",
        "# Specify a similarity threshold (adjust as needed)\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "# Remove redundancy\n",
        "deduplicated_text_entries = remove_redundancy(text_entries, similarity_threshold)\n",
        "\n",
        "# Print the deduplicated entries\n",
        "for entry in deduplicated_text_entries:\n",
        "    print(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmYql1rID4LW",
        "outputId": "1e2e03cd-4e35-47cd-a000-8378e347a063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example sentence.\n",
            "Another example sentence.\n",
            "Some unique text here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Cats and dogs are friends.\"\n",
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "3JGeZPOfZ94T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(tokens)\n",
        "one_hot_encoded = []\n",
        "for token in tokens:\n",
        "    one_hot_vector = [1 if token == word else 0 for word in vocab]\n",
        "    one_hot_encoded.append(one_hot_vector)\n"
      ],
      "metadata": {
        "id": "zw6_2y2cZ90_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODNIVXO0Z9yG",
        "outputId": "2a1d229b-b1f8-4a02-eb17-c268cb2667aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 0, 1],\n",
              " [0, 0, 1, 0, 0],\n",
              " [1, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_vector = [item for sublist in one_hot_encoded for item in sublist]\n",
        "print(flattened_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX3Z5VZCZ9vx",
        "outputId": "f4c4d477-1555-4300-d0f4-5bfb9eede902"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Tokenization is the process of breaking down a text into individual words or tokens.\"\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit the vectorizer on the text and transform the text into a one-hot encoded matrix\n",
        "one_hot_matrix = vectorizer.fit_transform([text])\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the one-hot matrix to a dense array\n",
        "one_hot_array = one_hot_matrix.toarray()\n",
        "\n",
        "# Create a dictionary to map words to their one-hot encoding\n",
        "one_hot_encoding = {word: one_hot_array[0][i] for i, word in enumerate(feature_names)}\n",
        "\n",
        "# Print the one-hot encoding for each word\n",
        "for word, encoding in one_hot_encoding.items():\n",
        "    print(f\"{word}: {encoding}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXj5itMHZ9tQ",
        "outputId": "b8140fc0-e3b8-4838-99fb-7207eeac5672"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breaking: 1\n",
            "down: 1\n",
            "individual: 1\n",
            "into: 1\n",
            "is: 1\n",
            "of: 1\n",
            "or: 1\n",
            "process: 1\n",
            "text: 1\n",
            "the: 1\n",
            "tokenization: 1\n",
            "tokens: 1\n",
            "words: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Tokenization is the process of breaking down a text into individual words or tokens.\"\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit the vectorizer on the text and transform the text into a one-hot encoded matrix\n",
        "one_hot_matrix = vectorizer.fit_transform([text])\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the one-hot matrix to a dense array\n",
        "one_hot_vectors = one_hot_matrix.toarray()\n",
        "\n",
        "# Create a dictionary to map words to their one-hot encoding\n",
        "one_hot_encoding = {word: vector for word, vector in zip(feature_names, one_hot_vectors)}\n",
        "\n",
        "# Print the one-hot encoding vectors for each word\n",
        "for word, encoding_vector in one_hot_encoding.items():\n",
        "    print(f\"{word}: {encoding_vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re1cartUZ9rI",
        "outputId": "b6d43cb5-a631-4747-b23c-bb38612416d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "breaking: [1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G-YJ8YHZ9nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4fmSzNqd6rgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5i5fMK-H4dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2O5DHpMSH4a-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}